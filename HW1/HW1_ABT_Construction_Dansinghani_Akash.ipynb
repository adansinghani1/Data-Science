{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Analytics Base Table Construciton\n",
    "---\n",
    "In this homework assignment, you will begin to explore the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM). \n",
    "\n",
    "\n",
    "Below you will find a number of steps that you will be required to complete before you can start the assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "---\n",
    "\n",
    "This assignment will only be using [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB), but we will be using more than one by the end of the semster. In later steps, you will need to access the uncompressed files from these partitions, so remember where you put them.\n",
    "\n",
    "A paper describing the construction of the dataset can be found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "---\n",
    "\n",
    "Individual partitions of the dataset can be accessed through following links:\n",
    "- [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB)\n",
    "- [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD)\n",
    "- [Partition 3](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/PTPGQT)\n",
    "- [Partition 4](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/FIFLFU)\n",
    "- [Partition 5](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/QC2C3X)\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Attributes:\n",
    "\n",
    "Each file in the dataset contains the following attributes as a single variate of the multivariate timeseries sample. \n",
    "\n",
    "|              |                  |             |\n",
    "|--------------|------------------|-------------|\n",
    "| 1. Timestamp | 2. TOTUSJH       | 3. TOTBSQ   |\t\n",
    "| 4. TOTPOT\t   | 5. TOTUSJZ       | 6. ABSNJZH  |\t\n",
    "| 7. SAVNCPP   | 8. USFLUX        | 9. TOTFZ\t|\n",
    "| 10. MEANPOT  | 11. EPSZ\t      | 12. MEANSHR |\n",
    "| 13. SHRGT45  | 14. MEANGAM      | 15. MEANGBT |\n",
    "| 16. MEANGBZ  | 17. MEANGBH      | 18. MEANJZH |\n",
    "| 19. TOTFY    | 20. MEANJZD      | 21. MEANALP |\t\n",
    "| 22. TOTFX    | 23. EPSY\t      | 24. EPSX\t|\n",
    "| 25. R_VALUE  | 26. CRVAL1       | 27. CRLN_OBS|\t\n",
    "| 28. CRLT_OBS | 29. CRVAL2       | 30. HC_ANGLE|\t\n",
    "| 31. SPEI     | 32. LAT_MIN      | 33. LON_MIN |\n",
    "| 34. LAT_MAX  | 35. LON_MAX      | 36. QUALITY |\t\n",
    "| 37. BFLARE   | 38. BFLARE_LABEL |\t39. CFLARE  |\t\n",
    "| 39. CFLARE_LABEL | 40. MFLARE | 41. MFLARE_LABEL |\t\n",
    "| 42. XFLARE | 43. XFLARE_LABEL | 44. BFLARE_LOC |\t\n",
    "| 45. BFLARE_LABEL_LOC | 46. CFLARE_LOC | 47. CFLARE_LABEL_LOC |\t\n",
    "| 48. MFLARE_LOC | 49. MFLARE_LABEL_LOC | 50. FLARE_LOC |\t\n",
    "| 51. XFLARE_LABEL_LOC | 52. XR_MAX | 53. XR_QUAL |\t\n",
    "|54. IS_TMFI | | |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Unpacking the data\n",
    "---\n",
    "\n",
    "The partitions come in tar.gz archive files. These are easily opened on all current operating systems using the same command in the terminal.\n",
    "\n",
    "- On Windows 10: Use cmd.exe, then run: tar xf partition1_instances.tar.gz\n",
    "- On Linux: In the terminal run: tar xf partition1_instances.tar.gz\n",
    "- On Mac: In the terminal run: tar xf partition1_instances.tar.gz\n",
    "\n",
    "These all assume you are in the directory that containes the tar.gz file and that you wish to unpack in this same directory.  Search for tar commands if you wish to do something else.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "---\n",
    "\n",
    "The __partition1__ direcotry contains two subdirectories, __FL__ and __NF__, these subdirectories represent the two classes of our target feature in the solar flare prediction problem we will be attempting to solve this semester. \n",
    "\n",
    "- __FL__: Represents the multivariate time series samples that have a Solar Flare occur within 24 hours of the observation.\n",
    "- __NF__: Represents the multivariate time series samples that do not have a Solar Flare occur within 24 hours of the observation.\n",
    "\n",
    "The multivariate time series samples are stored in .csv files for each individual sample. Each file name contains a number of pieces of information that we will wish to keep for our prediction task and therefore should be part of your Analytics Base Table. Below are examples of the naming for each sample type.\n",
    "\n",
    "- __FL__ file name example:M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv\n",
    "- __NF__ file name example:FQ_ar99_s2010-08-01T19:00:00_e2010-08-02T06:48:00.csv or B1.9@909:Primary_ar325_s2011-01-04T02:36:00_e2011-01-04T14:24:00.csv\n",
    "\n",
    "Let's look at these formats, starting with those that contain an @ symbol (we will use the __FL__ file as an example but note that the __NF__ data also has files with this naming):\n",
    "- __M1.0@265:Primary__: This says that there occurs an M1.0 sized flare within 24 hours of our sample. It also says that this flare is numbered 265 in the accompanying integrated flare dataset that comes as a supplementary file to this dataset. Additionally, \"Primary\" indicates that the intersection with this active region was verified through the primary method described in the paper.  \n",
    "- __\\_ar115__: This indicates which active region the sample comes from in the original unsampled dataset.\n",
    "- __\\_s2010-08-06T06:36:00__: This is the start time of the sample.\n",
    "- __\\_e2010-08-06T18:24:00.csv__: This is the ned time of the sample.\n",
    "\n",
    "The files that don't contain the @ symbol begin with FQ and do not have any flare occuring within 24 hours of the sample in the file.  __Note__ that both the __FL__ and __NF__ have files that have flares within 24 hours, but the __NF__ ones are smaller flares that we are considering as unimportant and therefore fall in the non-flaring class.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Reading the flare data (25 points)\n",
    "---\n",
    "\n",
    "Now that you have an understanding about the data, you will develop a method to read the flaring data and return an object that contains the data from the csv file and some of the information contained in the file name.\n",
    "\n",
    "Below is the object you will return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTSSample:\n",
    "    \n",
    "    def __init__(self, flare_type:str, start_time:datetime, end_time:datetime, data:DataFrame):\n",
    "        self._flare_type = flare_type\n",
    "        self._start_time = start_time\n",
    "        self._end_time = end_time\n",
    "        self._data = data\n",
    "    \n",
    "    def get_flare_type(self):\n",
    "        return self._flare_type\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self._start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self._end_time\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the MVTSSample class\n",
    "---\n",
    "\n",
    "The above class represents the data contained in one file. You are to return one of these objects for each call to your method(s). \n",
    "\n",
    "- The __flare_type__ is to be one of the following selections (__X__, __M__, __C__, __B__, __A__, __FQ__), and these lables will be derived from the information in the file name. \n",
    "- __start_time__ is the start time in the file name\n",
    "- __end_time__ is the end time in the file name\n",
    "- __data__ is a Pandas DataFrame which you will load from the csv using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About your method\n",
    "---\n",
    "\n",
    "Your method is to take in the path and name of the file to open, and it is to return one MVTSSample for that file.\n",
    "\n",
    "Below is a definition for that method, use it and write the code to complete the tasks necessary to return the specified information.  You can use a method call in another code block to test that your method works as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVTSSample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ac4b031c3f21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mread_flare_mvts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mMVTSSample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mflare_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'e'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MVTSSample' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "def read_flare_mvts(data_dir:str, file_name:str) -> MVTSSample:\n",
    "    flare_type = file_name[0]\n",
    "    start_time = file_name[file_name.find('s')+1:file_name.find('e')-1]\n",
    "    end_time = file_name[file_name.find('e')+1:file_name.find('v')-1]\n",
    "    data = pd.read_csv(data_dir, delimiter='\\t')\n",
    "    return MVTSSample(flare_type,start_time,end_time,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_flare_mvts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dfed9d67c119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\akash\\\\Desktop\\\\partition1\\\\FL'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_flare_mvts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'read_flare_mvts' is not defined"
     ]
    }
   ],
   "source": [
    "filepath = 'C:\\\\Users\\\\akash\\\\Desktop\\\\partition1\\\\FL\\\\M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv'\n",
    "data_dir = 'C:\\\\Users\\\\akash\\\\Desktop\\\\partition1\\\\FL'\n",
    "file_name =\"M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv\"\n",
    "results = read_flare_mvts(data_dir, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Reading the non-flare data (25 points)\n",
    "---\n",
    "\n",
    "Same as Question 1, but now you will do it for non-flaring data.\n",
    "\n",
    "Below is a definition for that method, use it and write the code to complete the tasks necessary to return the specified information. You can use a method call in another code block to test that your method works as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "def read_non_flare_mvts(data_dir:str, file_name:str) -> MVTSSample:\n",
    "    if file_name_1.startswith(\"F\"):\n",
    "            flare_type = pd.read_csv(file_name_1, usecols=['col1'])\n",
    "            \n",
    "    for file_name_1 in data_dir:\n",
    "            start_time = file_name.startswith(('__\\_s', '36:00__'))\n",
    "            end_time = file_name.startswith(('__\\_e', '24:00.csv__'))\n",
    "        else:\n",
    "            return file_name_1[0]\n",
    "        \n",
    "        for file_name_2 in data_dir:\n",
    "            if file_name_2.startswith(\"B\"):\n",
    "                return file_name_2[0]\n",
    "            else:\n",
    "                return re.search('_s(.+?)_e', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = glob.glob(\"partition1/NL/*.csv\")\n",
    "file_name_1 = \"FQ_ar99_s2010-08-01T19:00:00_e2010-08-02T06:48:00.csv\"\n",
    "file_name_2 = \"B1.9@909:Primary_ar325_s2011-01-04T02:36:00_e2011-01-04T14:24:00.csv\"\n",
    "results1 = read_non_flare_mvts(data_dir, file_name_1)\n",
    "results2 = read_non_flare_mvts(data_dir, file_name_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Processing the DataFrame (25 points)\n",
    "---\n",
    "\n",
    "Now that you can read individual files to get the multivariate time sries for a sample period, it is time to start building the analytics base table.\n",
    "\n",
    "The machine learning methods that we will cover this semster are generally applied to tabular data with a set of descriptive features that are used to learn to classify or predict a target feature. To accomplish this with our raw input multivariate time series, we must produce a set of descriptive features from each of the variates of the the time series.  \n",
    "\n",
    "In this quesion you will process the DataFrame that was returned from your previous two methods to construct a set of descriptive features for each sample. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DataFrame Attributes:\n",
    "\n",
    "Each file in the dataset contains the following attributes as a variate of the multivariate timeseries sample. \n",
    "\n",
    "|              |                  |             |\n",
    "|--------------|------------------|-------------|\n",
    "| 1. Timestamp | 2. TOTUSJH       | 3. TOTBSQ   |\t\n",
    "| 4. TOTPOT\t   | 5. TOTUSJZ       | 6. ABSNJZH  |\t\n",
    "| 7. SAVNCPP   | 8. USFLUX        | 9. TOTFZ\t|\n",
    "| 10. MEANPOT  | 11. EPSZ\t      | 12. MEANSHR |\n",
    "| 13. SHRGT45  | 14. MEANGAM      | 15. MEANGBT |\n",
    "| 16. MEANGBZ  | 17. MEANGBH      | 18. MEANJZH |\n",
    "| 19. TOTFY    | 20. MEANJZD      | 21. MEANALP |\t\n",
    "| 22. TOTFX    | 23. EPSY\t      | 24. EPSX\t|\n",
    "| 25. R_VALUE  | 26. CRVAL1       | 27. CRLN_OBS|\t\n",
    "| 28. CRLT_OBS | 29. CRVAL2       | 30. HC_ANGLE|\t\n",
    "| 31. SPEI     | 32. LAT_MIN      | 33. LON_MIN |\n",
    "| 34. LAT_MAX  | 35. LON_MAX      | 36. QUALITY |\t\n",
    "| 37. BFLARE   | 38. BFLARE_LABEL |\t39. CFLARE  |\t\n",
    "| 39. CFLARE_LABEL | 40. MFLARE | 41. MFLARE_LABEL |\t\n",
    "| 42. XFLARE | 43. XFLARE_LABEL | 44. BFLARE_LOC |\t\n",
    "| 45. BFLARE_LABEL_LOC | 46. CFLARE_LOC | 47. CFLARE_LABEL_LOC |\t\n",
    "| 48. MFLARE_LOC | 49. MFLARE_LABEL_LOC | 50. FLARE_LOC |\t\n",
    "| 51. XFLARE_LABEL_LOC | 52. XR_MAX | 53. XR_QUAL |\t\n",
    "|54. IS_TMFI | | |\n",
    "\n",
    "\n",
    "These columns should be present in your dataframe that was returned from your previous two methods.  We will only be utilizing a fraction of these.  The method description below gives you more information about which ones we will use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About your method\n",
    "---\n",
    "The following will be the variates we will be processing to return features of.\n",
    "\n",
    "|              |                  |             |\n",
    "|--------------|------------------|-------------|\n",
    "| 1. R_VALUE   | 2. TOTUSJH       | 3. TOTBSQ   |\t\n",
    "| 4. TOTPOT\t   | 5. TOTUSJZ       | 6. ABSNJZH  |\t\n",
    "| 7. SAVNCPP   | 8. USFLUX        | 9. TOTFZ\t|\n",
    "| 10. MEANPOT  | 11. EPSZ\t      | 12. MEANSHR |\n",
    "| 13. SHRGT45  | 14. MEANGAM      | 15. MEANGBT |\n",
    "| 16. MEANGBZ  | 17. MEANGBH      | 18. MEANJZH |\n",
    "| 19. TOTFY    | 20. MEANJZD      | 21. MEANALP |\t\n",
    "| 22. TOTFX    |        \t      |         \t|\n",
    "\n",
    "For each of these variates you will calculate two descriptive features: \n",
    "- Mean \n",
    "- Standard Deviation\n",
    "\n",
    "We will add more later, but for now, this will be sufficient to demonstrate the analytics base table construction process.\n",
    "\n",
    "Below is a method defintion, complete it to return the above specified information. You can use a method call in another code block to test that your method works as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def calculate_descriptive_features(data:DataFrame)-> DataFrame:\n",
    "    variates_to_calc_on = [ 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                           'USFLUX','TOTFZ','MEANPOT','EPSZ','MEANSHR','SHRGT45','MEANGAM','MEANGBT',\n",
    "                           'MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    features_to_return = [ 'R_VALUE_MEAN','R_VALUE_STDDEV',\n",
    "                          'TOTUSJH_MEAN','TOTUSJH_STDDEV',\n",
    "                          'TOTBSQ_MEAN','TOTBSQ_STDDEV',\n",
    "                          'TOTPOT_MEAN','TOTPOT_STDDEV',\n",
    "                          'TOTUSJZ_MEAN','TOTUSJZ_STDDEV',\n",
    "                          'ABSNJZH_MEAN','ABSNJZH_STDDEV',\n",
    "                          'SAVNCPP_MEAN','SAVNCPP_STDDEV',\n",
    "                          'USFLUX_MEAN','USFLUX_STDDEV',\n",
    "                          'TOTFZ_MEAN','TOTFZ_STDDEV',\n",
    "                          'MEANPOT_MEAN','MEANPOT_STDDEV',\n",
    "                          'EPSZ_MEAN','EPSZ_STDDEV',\n",
    "                          'MEANSHR_MEAN','MEANSHR_STDDEV',\n",
    "                          'SHRGT45_MEAN','SHRGT45_STDDEV',\n",
    "                          'MEANGAM_MEAN','MEANGAM_STDDEV',\n",
    "                          'MEANGBT_MEAN','MEANGBT_STDDEV',\n",
    "                          'MEANGBZ_MEAN','MEANGBZ_STDDEV',\n",
    "                          'MEANGBH_MEAN','MEANGBH_STDDEV',\n",
    "                          'MEANJZH_MEAN','MEANJZH_STDDEV',\n",
    "                          'TOTFY_MEAN','TOTFY_STDDEV',\n",
    "                          'MEANJZD_MEAN','MEANJZD_STDDEV',\n",
    "                          'MEANALP_MEAN','MEANALP_STDDEV',\n",
    "                          'TOTFX_MEAN','TOTFX_STDDEV']\n",
    "    print(\"Make sure to calculate\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    import os\n",
    "    import csv\n",
    "    dataColumns = pd.DataFrame(columns=features_to_return)\n",
    "    boolean = True\n",
    "\n",
    "    for i in cols:\n",
    "        try:\n",
    "            mean = data[i].mean()\n",
    "            std = data[i].std()\n",
    "            avgStr = i+'_MEAN'\n",
    "            devStr = i+'_STDDEV'\n",
    "            dataColumns.at[0,avgStr] = mean\n",
    "            dataColumns.at[0,devStr] = std            \n",
    "        except:\n",
    "            boolean = False \n",
    "        return dataColumns,boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Putting it all together (25 points)\n",
    "---\n",
    "\n",
    "Now that you have to tools to read the data and process descriptive features, it is time to put this all together to produce an analytics base table for all of the data in Partiton 1.\n",
    "\n",
    "In this question, you shall construct a method that will process a partition by extracting features for each sample in both the __FL__ and __NF__ subdirectories of that partition.  The extracted descriptive features are to be placed into your analytics base table DataFrame as colums, with the addition of the __FLARE_TYPE__ target feature.\n",
    "\n",
    "Your method should take in the partition location and assume that there will be __FL__ and __NF__ subdirectories to process.\n",
    "\n",
    "Your method shall also take in the name of the analytics base table to store. This should be the full name with either an absolute or relative path to store the table also part of the passed in name. \n",
    "\n",
    "Below you will find a method defintion, complete it to perform the above specified information. You can use a method call in another code block to test that your method works as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import glob\n",
    "def process_partition(partition_location:str, abt_name:str):\n",
    "    abt_header = [ 'FLARE_TYPE', 'R_VALUE_MEAN','R_VALUE_STDDEV',\n",
    "                          'TOTUSJH_MEAN','TOTUSJH_STDDEV',\n",
    "                          'TOTBSQ_MEAN','TOTBSQ_STDDEV',\n",
    "                          'TOTPOT_MEAN','TOTPOT_STDDEV',\n",
    "                          'TOTUSJZ_MEAN','TOTUSJZ_STDDEV',\n",
    "                          'ABSNJZH_MEAN','ABSNJZH_STDDEV',\n",
    "                          'SAVNCPP_MEAN','SAVNCPP_STDDEV',\n",
    "                          'USFLUX_MEAN','USFLUX_STDDEV',\n",
    "                          'TOTFZ_MEAN','TOTFZ_STDDEV',\n",
    "                          'MEANPOT_MEAN','MEANPOT_STDDEV',\n",
    "                          'EPSZ_MEAN','EPSZ_STDDEV',\n",
    "                          'MEANSHR_MEAN','MEANSHR_STDDEV',\n",
    "                          'SHRGT45_MEAN','SHRGT45_STDDEV',\n",
    "                          'MEANGAM_MEAN','MEANGAM_STDDEV',\n",
    "                          'MEANGBT_MEAN','MEANGBT_STDDEV',\n",
    "                          'MEANGBZ_MEAN','MEANGBZ_STDDEV',\n",
    "                          'MEANGBH_MEAN','MEANGBH_STDDEV',\n",
    "                          'MEANJZH_MEAN','MEANJZH_STDDEV',\n",
    "                          'TOTFY_MEAN','TOTFY_STDDEV',\n",
    "                          'MEANJZD_MEAN','MEANJZD_STDDEV',\n",
    "                          'MEANALP_MEAN','MEANALP_STDDEV',\n",
    "                          'TOTFX_MEAN','TOTFX_STDDEV']\n",
    "    abt = DataFrame(columns=abt_header)\n",
    "    conc = []\n",
    "    import glob\n",
    "    csvFile = + \"*.csv\"\n",
    "    tmp = partition_location + csvFile\n",
    "    files = glob.glob(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
